{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d000b1-bb3e-4322-8e90-22a9c40f463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --only-binary=:all: numpy==1.24.3\n",
    "!pip install --only-binary=:all: pandas scipy scikit-learn\n",
    "!pip install --only-binary=:all: lightgbm xgboost catboost\n",
    "!pip install --only-binary=:all: imbalanced-learn optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1964fa-be2c-444c-9ee9-162ae11baecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import optuna\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (classification_report, roc_auc_score, roc_curve,\n",
    "                              precision_recall_curve, average_precision_score,\n",
    "                              precision_score, recall_score, f1_score, confusion_matrix)\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d684742-06d9-4e2f-ba9d-ddabd61b1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b24fa85-3ae5-49a0-bfcc-25178bd9fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_path = \"/Users/pratyushmalviya/Desktop/fraud final model 26:4:25/creditcard.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d4aa5",
   "metadata": {},
   "source": [
    "Exploratory data analysis-\n",
    "performing  a basic exploratory analysis on the dataframe , particularly focusing on class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca630b3b-d137-4ccd-a7c8-c19a209ee1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPLORATORY DATA ANALYSIS ===\n",
      "Dataset shape: (284807, 31)\n",
      "Missing values: 0\n",
      "\n",
      "Class Distribution:\n",
      "Class 0: 284315 samples (99.827%)\n",
      "Class 1: 492 samples (0.173%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EXPLORATORY DATA ANALYSIS ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "class_counts = df['Class'].value_counts()\n",
    "total = len(df)\n",
    "print(\"\\nClass Distribution:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"Class {cls}: {count} samples ({count/total*100:.3f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdfca8b",
   "metadata": {},
   "source": [
    "Time feature analysis - \n",
    "To identify hourly patterns in fraudulent behavior, which can inform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab865d89-323f-493a-8dd2-2f4cbb475e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Time Features Analysis ===\n",
      "\n",
      "Fraud Transactions by Hour:\n",
      "Hour\n",
      "0.0      2\n",
      "1.0      2\n",
      "2.0     21\n",
      "3.0     13\n",
      "4.0      6\n",
      "5.0     11\n",
      "6.0      3\n",
      "7.0     23\n",
      "8.0      5\n",
      "9.0     15\n",
      "10.0     2\n",
      "11.0    43\n",
      "12.0     9\n",
      "13.0     9\n",
      "14.0    13\n",
      "15.0    14\n",
      "16.0    14\n",
      "17.0    12\n",
      "18.0    15\n",
      "19.0     7\n",
      "20.0     8\n",
      "21.0    14\n",
      "22.0     3\n",
      "23.0    17\n",
      "24.0     4\n",
      "25.0     8\n",
      "26.0    36\n",
      "27.0     4\n",
      "28.0    17\n",
      "30.0     6\n",
      "32.0     4\n",
      "33.0     1\n",
      "34.0     6\n",
      "35.0    10\n",
      "36.0     8\n",
      "37.0     8\n",
      "38.0    10\n",
      "39.0    12\n",
      "40.0     8\n",
      "41.0    17\n",
      "42.0    18\n",
      "43.0    12\n",
      "44.0    10\n",
      "45.0     2\n",
      "46.0     6\n",
      "47.0     4\n",
      "dtype: int64\n",
      "\n",
      "Fraud Rate by Hour (%):\n",
      "Hour\n",
      "0.0     0.050467\n",
      "1.0     0.090212\n",
      "2.0     1.332487\n",
      "3.0     0.713893\n",
      "4.0     0.554529\n",
      "5.0     0.654372\n",
      "6.0     0.163845\n",
      "7.0     0.682898\n",
      "8.0     0.096544\n",
      "9.0     0.190404\n",
      "10.0    0.024131\n",
      "11.0    0.504873\n",
      "12.0    0.116399\n",
      "13.0    0.118655\n",
      "14.0    0.161913\n",
      "15.0    0.178663\n",
      "16.0    0.179810\n",
      "17.0    0.152246\n",
      "18.0    0.174277\n",
      "19.0    0.087566\n",
      "20.0    0.089087\n",
      "21.0    0.141486\n",
      "22.0    0.033419\n",
      "23.0    0.279513\n",
      "24.0    0.107181\n",
      "25.0    0.399401\n",
      "26.0    2.054795\n",
      "27.0    0.239378\n",
      "28.0    1.508429\n",
      "29.0    0.000000\n",
      "30.0    0.264317\n",
      "31.0    0.000000\n",
      "32.0    0.078478\n",
      "33.0    0.012563\n",
      "34.0    0.072202\n",
      "35.0    0.119918\n",
      "36.0    0.104058\n",
      "37.0    0.102828\n",
      "38.0    0.117082\n",
      "39.0    0.139130\n",
      "40.0    0.092304\n",
      "41.0    0.205215\n",
      "42.0    0.213472\n",
      "43.0    0.156760\n",
      "44.0    0.128601\n",
      "45.0    0.025615\n",
      "46.0    0.092822\n",
      "47.0    0.082372\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'Time' in df.columns:\n",
    "\n",
    "    df['Hour'] = df['Time'] // 3600\n",
    "    fraud_by_hour = df[df['Class'] == 1].groupby('Hour').size()\n",
    "    hourly_fraud_rate = df.groupby('Hour')['Class'].mean() * 100\n",
    "\n",
    "    print(\"\\n=== Time Features Analysis ===\")\n",
    "    print(\"\\nFraud Transactions by Hour:\")\n",
    "    print(fraud_by_hour)\n",
    "\n",
    "    print(\"\\nFraud Rate by Hour (%):\")\n",
    "    print(hourly_fraud_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5014ac7",
   "metadata": {},
   "source": [
    "AMOUNT DISTRIBUTION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb3128-33d2-461b-807a-bea01006e1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Amount Distribution Analysis ===\n",
      "\n",
      "Transaction Amount Statistics by Class:\n",
      "          count        mean         std  min   25%    50%     75%       max\n",
      "Class                                                                      \n",
      "0      284315.0   88.291022  250.105092  0.0  5.65  22.00   77.05  25691.16\n",
      "1         492.0  122.211321  256.683288  0.0  1.00   9.25  105.89   2125.87\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Amount Distribution Analysis ===\")\n",
    "amount_stats = df.groupby('Class')['Amount'].describe()\n",
    "print(\"\\nTransaction Amount Statistics by Class:\")\n",
    "print(amount_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d268db",
   "metadata": {},
   "source": [
    " Correlation Matrix of Anonymous Features-\n",
    " To understand how the anonymized features are related to each other, which helps\n",
    "\n",
    "1) Detect multicollinearity.\n",
    "2) Guide dimensionality reduction or feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068da06-f540-423b-86bc-4fc8b675444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer to create new features\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mean_amount = None\n",
    "        self.std_amount = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if 'Amount' in X.columns:\n",
    "            self.mean_amount = X['Amount'].mean()\n",
    "            self.std_amount = X['Amount'].std()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        \n",
    "        if 'Time' in X_new.columns:\n",
    "            X_new['Hour'] = X_new['Time'] // 3600 % 24\n",
    "            X_new['Hour_sin'] = np.sin(2 * np.pi * X_new['Hour'] / 24)\n",
    "            X_new['Hour_cos'] = np.cos(2 * np.pi * X_new['Hour'] / 24)\n",
    "\n",
    "            X_new['IsMorning'] = ((X_new['Hour'] >= 6) & (X_new['Hour'] < 12)).astype(int)\n",
    "            X_new['IsAfternoon'] = ((X_new['Hour'] >= 12) & (X_new['Hour'] < 18)).astype(int)\n",
    "            X_new['IsEvening'] = ((X_new['Hour'] >= 18) & (X_new['Hour'] < 22)).astype(int)\n",
    "            X_new['IsNight'] = ((X_new['Hour'] >= 22) | (X_new['Hour'] < 6)).astype(int)\n",
    "            X_new = X_new.drop(['Time', 'Hour'], axis=1)\n",
    "        \n",
    "        if 'Amount' in X_new.columns:\n",
    "            X_new['Amount_Log'] = np.log1p(X_new['Amount'])\n",
    "            X_new['Amount_Zscore'] = (X_new['Amount'] - self.mean_amount) / self.std_amount\n",
    "\n",
    "            X_new['IsSmallTxn'] = (X_new['Amount'] <= 5).astype(int)\n",
    "            X_new['IsMediumTxn'] = ((X_new['Amount'] > 5) & (X_new['Amount'] <= 100)).astype(int)\n",
    "            X_new['IsLargeTxn'] = (X_new['Amount'] > 100).astype(int)\n",
    "\n",
    "            X_new = X_new.drop(['Amount'], axis=1)\n",
    "        \n",
    "        #  features engineering\n",
    "        v_columns = [col for col in X_new.columns if col.startswith('V')]\n",
    "        if v_columns:\n",
    "\n",
    "            X_new['V_Sum'] = X_new[v_columns].sum(axis=1)\n",
    "            X_new['V_Mean'] = X_new[v_columns].mean(axis=1)\n",
    "            X_new['V_Std'] = X_new[v_columns].std(axis=1)\n",
    "            X_new['V_Kurtosis'] = X_new[v_columns].kurtosis(axis=1)\n",
    "            X_new['V_Skew'] = X_new[v_columns].skew(axis=1)\n",
    "            \n",
    "            # Feature ratios (using most important V features based on common findings)\n",
    "            if all(col in X_new.columns for col in ['V1', 'V3', 'V4', 'V10', 'V11']):\n",
    "                X_new['V1_to_V3'] = X_new['V1'] / (X_new['V3'] + 1e-8)\n",
    "                X_new['V4_to_V10'] = X_new['V4'] / (X_new['V10'] + 1e-8)\n",
    "                X_new['V11_to_V4'] = X_new['V11'] / (X_new['V4'] + 1e-8)\n",
    "        \n",
    "        return X_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688aa902",
   "metadata": {},
   "source": [
    "enhanced dataframe-\n",
    "\n",
    "Applies the FeatureEngineer transformer to create new features, enhancing the dataset (df_enhanced).\n",
    "\n",
    "Computes and visualizes the top 20 features with the strongest absolute correlations to the target variable 'Class' using a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a2f56-3f48-4b67-b403-4ae1fc61451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== APPLYING FEATURE ENGINEERING ===\n",
      "\n",
      "Top 20 Features Most Correlated with Fraud (absolute values):\n",
      "V17       0.326481\n",
      "V_Mean    0.316330\n",
      "V_Sum     0.316330\n",
      "V14       0.302544\n",
      "V12       0.260593\n",
      "V_Std     0.250839\n",
      "V10       0.216883\n",
      "V16       0.196539\n",
      "V3        0.192961\n",
      "V7        0.187257\n",
      "V11       0.154876\n",
      "V4        0.133447\n",
      "V18       0.111485\n",
      "V1        0.101347\n",
      "V9        0.097733\n",
      "V5        0.094974\n",
      "V2        0.091289\n",
      "V6        0.043643\n",
      "V21       0.040413\n",
      "V_Skew    0.035339\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== APPLYING FEATURE ENGINEERING ===\")\n",
    "feature_engineer = FeatureEngineer()\n",
    "df_enhanced = feature_engineer.fit_transform(df)\n",
    "if 'Class' in df_enhanced.columns:\n",
    "    correlations = df_enhanced.corrwith(df_enhanced['Class']).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 20 Features Most Correlated with Fraud (absolute values):\")\n",
    "    top_features = correlations.drop('Class').abs().sort_values(ascending=False).head(20)\n",
    "    print(top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6565fa",
   "metadata": {},
   "source": [
    "DATA PREPARATION AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1f3a29e-98bd-4840-8067-0bd65f1fa082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features shape after engineering: (284807, 47)\n",
      "Training set: (199364, 47), Test set: (85443, 47)\n"
     ]
    }
   ],
   "source": [
    "# Split into features and target\n",
    "X = df_enhanced.drop(['Class'], axis=1) if 'Class' in df_enhanced.columns else df_enhanced\n",
    "y = df['Class'] if 'Class' in df else None\n",
    "\n",
    "print(f\"\\nFeatures shape after engineering: {X.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47acfb02",
   "metadata": {},
   "source": [
    "Define preprocessing pipeline with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09a26b-3f12-4e9c-a32d-caa9a19d0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tempfile import mkdtemp\n",
    "from joblib import Memory\n",
    "cachedir = mkdtemp()\n",
    "memory = Memory(location=cachedir, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae45f7",
   "metadata": {},
   "source": [
    "Preprocessing Pipeline: Applies PowerTransformer (Yeo-Johnson method) to scale the training and test data (X_train, X_test).\n",
    "\n",
    "PCA Transformation: Reduces dimensionality using PCA to retain 95% variance, creating new PCA features.\n",
    "\n",
    "Combined Features: Combines the original preprocessed features with the PCA components, resulting in a new feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1859cfbf-5c57-4c12-905d-9dbe43a51bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== APPLYING PREPROCESSING ===\n",
      "Number of PCA components: 35\n",
      "Final feature count: 82\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing pipeline\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('scaler', PowerTransformer(method='yeo-johnson'))\n",
    "])\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"\\n=== APPLYING PREPROCESSING ===\")\n",
    "X_train_preprocessed = preprocess_pipeline.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "# Add PCA features \n",
    "pca = PCA(n_components=0.95, random_state=42)  # Keep 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train_preprocessed)\n",
    "X_test_pca = pca.transform(X_test_preprocessed)\n",
    "\n",
    "print(f\"Number of PCA components: {pca.n_components_}\")\n",
    "\n",
    "# Combine original preprocessed features with PCA features\n",
    "X_train_with_pca = np.hstack((X_train_preprocessed, X_train_pca))\n",
    "X_test_with_pca = np.hstack((X_test_preprocessed, X_test_pca))\n",
    "\n",
    "# Create feature names for PCA components\n",
    "original_feature_names = list(X.columns)\n",
    "pca_feature_names = [f'PCA_{i}' for i in range(pca.n_components_)]\n",
    "all_feature_names = original_feature_names + pca_feature_names\n",
    "\n",
    "print(f\"Final feature count: {len(all_feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07631faa",
   "metadata": {},
   "source": [
    "Handling class imbalance -\n",
    "\n",
    "Techniques: None, SMOTE, ADASYN, SMOTETomek, Random Undersampling.\n",
    "\n",
    "Performance evaluated using an XGBClassifier and metrics: Precision, Recall, F1 Score, PR-AUC, ROC-AUC.\n",
    "\n",
    "Best Technique Selection:\n",
    "\n",
    "The best resampling technique is selected based on the highest PR-AUC score.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78c508-ee74-4b65-9646-0603b66a3130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATING RESAMPLING TECHNIQUES ===\n",
      "\n",
      "Evaluating: None\n",
      "Original class distribution: {0: 199020, 1: 344}\n",
      "Ratio (Fraud/Normal): 0.0017\n",
      "Test metrics - Precision: 0.9576, Recall: 0.7635, F1: 0.8496\n",
      "PR-AUC: 0.8304, ROC-AUC: 0.9712\n",
      "\n",
      "Evaluating: SMOTE\n",
      "Class distribution after resampling: {0: 199020, 1: 19902}\n",
      "Ratio (Fraud/Normal): 0.1000\n",
      "Test metrics - Precision: 0.7756, Recall: 0.8176, F1: 0.7961\n",
      "PR-AUC: 0.8253, ROC-AUC: 0.9705\n",
      "\n",
      "Evaluating: ADASYN\n",
      "Class distribution after resampling: {0: 199020, 1: 19894}\n",
      "Ratio (Fraud/Normal): 0.1000\n",
      "Test metrics - Precision: 0.6780, Recall: 0.8108, F1: 0.7385\n",
      "PR-AUC: 0.7983, ROC-AUC: 0.9715\n",
      "\n",
      "Evaluating: SMOTETomek\n",
      "Class distribution after resampling: {0: 199020, 1: 19902}\n",
      "Ratio (Fraud/Normal): 0.1000\n",
      "Test metrics - Precision: 0.7756, Recall: 0.8176, F1: 0.7961\n",
      "PR-AUC: 0.8253, ROC-AUC: 0.9705\n",
      "\n",
      "Evaluating: Random Undersampling\n",
      "Class distribution after resampling: {0: 688, 1: 344}\n",
      "Ratio (Fraud/Normal): 0.5000\n",
      "Test metrics - Precision: 0.1148, Recall: 0.8581, F1: 0.2026\n",
      "PR-AUC: 0.7597, ROC-AUC: 0.9649\n",
      "\n",
      "=== RESAMPLING TECHNIQUES COMPARISON ===\n",
      "              Technique  F1 Score    PR-AUC  Precision    Recall   ROC-AUC\n",
      "0                  None  0.849624  0.830387   0.957627  0.763514  0.971198\n",
      "1                 SMOTE  0.796053  0.825312   0.775641  0.817568  0.970522\n",
      "3            SMOTETomek  0.796053  0.825312   0.775641  0.817568  0.970522\n",
      "2                ADASYN  0.738462  0.798335   0.677966  0.810811  0.971488\n",
      "4  Random Undersampling  0.202552  0.759743   0.114828  0.858108  0.964864\n",
      "\n",
      "Best resampling technique: None with PR-AUC of 0.8304\n"
     ]
    }
   ],
   "source": [
    "# === Define resampling techniques ===\n",
    "print(\"\\n=== EVALUATING RESAMPLING TECHNIQUES ===\")\n",
    "\n",
    "resampling_techniques = {\n",
    "    'None': None,\n",
    "    'SMOTE': SMOTE(sampling_strategy=0.1, random_state=42),\n",
    "    'ADASYN': ADASYN(sampling_strategy=0.1, random_state=42),\n",
    "    'SMOTETomek': SMOTETomek(sampling_strategy=0.1, random_state=42),\n",
    "    'Random Undersampling': RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "}\n",
    "\n",
    "def evaluate_resampler(name, resampler, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Evaluate different resampling techniques\"\"\"\n",
    "    print(f\"\\nEvaluating: {name}\")\n",
    "    \n",
    "    if resampler is not None:\n",
    "        X_res, y_res = resampler.fit_resample(X_train, y_train)\n",
    "        unique, counts = np.unique(y_res, return_counts=True)\n",
    "        print(f\"Class distribution after resampling: {dict(zip(unique, counts))}\")\n",
    "        print(f\"Ratio (Fraud/Normal): {counts[1]/counts[0]:.4f}\")\n",
    "    else:\n",
    "        X_res, y_res = X_train, y_train\n",
    "        unique, counts = np.unique(y_res, return_counts=True)\n",
    "        print(f\"Original class distribution: {dict(zip(unique, counts))}\")\n",
    "        print(f\"Ratio (Fraud/Normal): {counts[1]/counts[0]:.4f}\")\n",
    "\n",
    "    # Train a quick model to evaluate\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=1,\n",
    "        random_state=42,\n",
    "        n_jobs=4,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Split into train/validation\n",
    "    X_res_train, X_res_val, y_res_train, y_res_val = train_test_split(\n",
    "        X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    clf.fit(\n",
    "        X_res_train, y_res_train,\n",
    "        eval_set=[(X_res_val, y_res_val)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    print(f\"Test metrics - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    print(f\"PR-AUC: {pr_auc:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Garbage collection\n",
    "    del clf, X_res_train, X_res_val, y_res_train, y_res_val\n",
    "    gc.collect()\n",
    "\n",
    "    return name, (f1, pr_auc, precision, recall, roc_auc), (X_res, y_res)\n",
    "\n",
    "\n",
    "resampling_results = {}\n",
    "resampled_data = {}\n",
    "\n",
    "for name, resampler in resampling_techniques.items():\n",
    "    name, metrics, data = evaluate_resampler(\n",
    "        name, resampler, X_train_with_pca, y_train, X_test_with_pca, y_test\n",
    "    )\n",
    "    resampling_results[name] = metrics\n",
    "    resampled_data[name] = data\n",
    "\n",
    "# === Create results DataFrame ===\n",
    "resampling_df = pd.DataFrame(columns=['Technique', 'F1 Score', 'PR-AUC', 'Precision', 'Recall', 'ROC-AUC'])\n",
    "\n",
    "for name, metrics in resampling_results.items():\n",
    "    resampling_df = pd.concat([resampling_df, pd.DataFrame({\n",
    "        'Technique': [name],\n",
    "        'F1 Score': [metrics[0]],\n",
    "        'PR-AUC': [metrics[1]],\n",
    "        'Precision': [metrics[2]],\n",
    "        'Recall': [metrics[3]],\n",
    "        'ROC-AUC': [metrics[4]]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== RESAMPLING TECHNIQUES COMPARISON ===\")\n",
    "print(resampling_df.sort_values('PR-AUC', ascending=False))\n",
    "\n",
    "# === Select best method by PR-AUC ===\n",
    "best_resampling = resampling_df.loc[resampling_df['PR-AUC'].idxmax(), 'Technique']\n",
    "print(f\"\\nBest resampling technique: {best_resampling} with PR-AUC of {resampling_df['PR-AUC'].max():.4f}\")\n",
    "\n",
    "# Get resampled data for best technique\n",
    "X_train_resampled, y_train_resampled = resampled_data[best_resampling]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b495980",
   "metadata": {},
   "source": [
    " MODEL TUNING & OPTIMIZATION-\n",
    "\n",
    " Models Optimized: XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "Objective: Maximizing PR-AUC (Precision-Recall AUC) for each model.\n",
    "\n",
    "Trials: 35 trials per model to optimize hyperparameters efficiently.\n",
    "\n",
    "XGBoost: Optimizes parameters like max_depth, learning_rate, min_child_weight, subsample, etc., using cross-validation and early stopping.\n",
    "\n",
    "LightGBM: Focuses on parameters such as num_leaves, learning_rate, max_depth, subsample, etc.\n",
    "\n",
    "CatBoost: Tunes parameters like iterations, depth, learning_rate, l2_leaf_reg, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6be64-584b-4e16-8894-a48121f965f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 11:56:33,733] A new study created in memory with name: no-name-a0b347be-f4af-4eeb-99c2-3a5eba5f3f45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STARTING MODEL OPTIMIZATION ===\n",
      "Data loaded successfully. X shape: (199364, 82), y shape: (199364,)\n",
      "\n",
      "Optimizing XGBoost hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 11:56:35,328] Trial 0 finished with value: 0.7809170080680178 and parameters: {'max_depth': 8, 'learning_rate': 0.03266840172964194, 'min_child_weight': 4, 'subsample': 0.9502102487642593, 'colsample_bytree': 0.7668468829054276, 'scale_pos_weight': 19.15354004030622, 'gamma': 3.476308254713722, 'reg_alpha': 3.65834066846962, 'reg_lambda': 4.319090585478796, 'n_estimators': 115}. Best is trial 0 with value: 0.7809170080680178.\n",
      "[I 2025-04-26 11:56:37,766] Trial 1 finished with value: 0.8269750499896817 and parameters: {'max_depth': 7, 'learning_rate': 0.10976892362347737, 'min_child_weight': 1, 'subsample': 0.8915516033395161, 'colsample_bytree': 0.7213636873023949, 'scale_pos_weight': 7.417629894045873, 'gamma': 1.2780733840700176, 'reg_alpha': 4.756668486853968, 'reg_lambda': 3.0749487582805473, 'n_estimators': 111}. Best is trial 1 with value: 0.8269750499896817.\n",
      "[I 2025-04-26 11:56:38,917] Trial 2 finished with value: 0.7632392469198009 and parameters: {'max_depth': 3, 'learning_rate': 0.020425967848282107, 'min_child_weight': 4, 'subsample': 0.9180550854180018, 'colsample_bytree': 0.6631503238265591, 'scale_pos_weight': 6.068883750821831, 'gamma': 2.334482247883675, 'reg_alpha': 1.7588680885196277, 'reg_lambda': 1.2704045200751224, 'n_estimators': 222}. Best is trial 1 with value: 0.8269750499896817.\n",
      "[I 2025-04-26 11:56:41,106] Trial 3 finished with value: 0.7986723325973685 and parameters: {'max_depth': 5, 'learning_rate': 0.033960690302528455, 'min_child_weight': 5, 'subsample': 0.825109091942701, 'colsample_bytree': 0.8974843612886544, 'scale_pos_weight': 4.6385360601665, 'gamma': 4.450123524455974, 'reg_alpha': 2.057626304492766, 'reg_lambda': 1.104212326034388, 'n_estimators': 172}. Best is trial 1 with value: 0.8269750499896817.\n",
      "[I 2025-04-26 11:56:45,052] Trial 4 finished with value: 0.7776994342147753 and parameters: {'max_depth': 6, 'learning_rate': 0.011479986289962546, 'min_child_weight': 5, 'subsample': 0.9226039735539782, 'colsample_bytree': 0.6878391493869348, 'scale_pos_weight': 14.589796114753273, 'gamma': 3.0818368657295614, 'reg_alpha': 0.3456732460790096, 'reg_lambda': 0.7509021540668659, 'n_estimators': 104}. Best is trial 1 with value: 0.8269750499896817.\n",
      "[I 2025-04-26 11:56:49,712] Trial 5 finished with value: 0.7778923120070669 and parameters: {'max_depth': 9, 'learning_rate': 0.0540564231005347, 'min_child_weight': 10, 'subsample': 0.6728576395072161, 'colsample_bytree': 0.8818159250287114, 'scale_pos_weight': 13.091021514724094, 'gamma': 3.6974673266017843, 'reg_alpha': 0.7439756369623385, 'reg_lambda': 0.29673351813061133, 'n_estimators': 147}. Best is trial 1 with value: 0.8269750499896817.\n",
      "[I 2025-04-26 11:56:53,322] Trial 6 finished with value: 0.8533524662314643 and parameters: {'max_depth': 9, 'learning_rate': 0.16034840607940198, 'min_child_weight': 7, 'subsample': 0.792194748187588, 'colsample_bytree': 0.7980667500160504, 'scale_pos_weight': 6.074632502477418, 'gamma': 4.354484742351192, 'reg_alpha': 1.8865756242866256, 'reg_lambda': 4.514260155150907, 'n_estimators': 265}. Best is trial 6 with value: 0.8533524662314643.\n",
      "[I 2025-04-26 11:56:54,910] Trial 7 finished with value: 0.7964810185549953 and parameters: {'max_depth': 5, 'learning_rate': 0.04145957758999652, 'min_child_weight': 5, 'subsample': 0.6573099741884451, 'colsample_bytree': 0.8516213233509089, 'scale_pos_weight': 2.507023378602818, 'gamma': 4.15311113734248, 'reg_alpha': 3.480664268044256, 'reg_lambda': 0.39947425751186316, 'n_estimators': 290}. Best is trial 6 with value: 0.8533524662314643.\n",
      "[I 2025-04-26 11:56:56,177] Trial 8 finished with value: 0.7711597730453725 and parameters: {'max_depth': 6, 'learning_rate': 0.0203906004106553, 'min_child_weight': 5, 'subsample': 0.6430182094098535, 'colsample_bytree': 0.8813828853917134, 'scale_pos_weight': 19.62909044099377, 'gamma': 1.7510860744748702, 'reg_alpha': 2.1763710745403535, 'reg_lambda': 1.6367549375772672, 'n_estimators': 134}. Best is trial 6 with value: 0.8533524662314643.\n",
      "[I 2025-04-26 11:56:57,956] Trial 9 finished with value: 0.7927463929074188 and parameters: {'max_depth': 7, 'learning_rate': 0.04797544396323872, 'min_child_weight': 8, 'subsample': 0.8313919493660917, 'colsample_bytree': 0.9620114399868058, 'scale_pos_weight': 10.544668613482147, 'gamma': 3.936454004662286, 'reg_alpha': 0.1054843884118506, 'reg_lambda': 0.6419911032743986, 'n_estimators': 277}. Best is trial 6 with value: 0.8533524662314643.\n",
      "[I 2025-04-26 11:57:01,247] Trial 10 finished with value: 0.858490506417675 and parameters: {'max_depth': 9, 'learning_rate': 0.1701967057986894, 'min_child_weight': 8, 'subsample': 0.7600837671631735, 'colsample_bytree': 0.621899799644963, 'scale_pos_weight': 2.1124290277314106, 'gamma': 0.21632632343556457, 'reg_alpha': 1.2803655298431948, 'reg_lambda': 4.99369803651072, 'n_estimators': 226}. Best is trial 10 with value: 0.858490506417675.\n",
      "[I 2025-04-26 11:57:04,927] Trial 11 finished with value: 0.8615849128293896 and parameters: {'max_depth': 9, 'learning_rate': 0.19204590754392462, 'min_child_weight': 8, 'subsample': 0.7395244220349267, 'colsample_bytree': 0.6106090410074912, 'scale_pos_weight': 1.5603241849262437, 'gamma': 0.5971932711844856, 'reg_alpha': 1.1856237564368213, 'reg_lambda': 4.949550158662442, 'n_estimators': 218}. Best is trial 11 with value: 0.8615849128293896.\n",
      "[I 2025-04-26 11:57:07,619] Trial 12 finished with value: 0.8513239770038504 and parameters: {'max_depth': 9, 'learning_rate': 0.18838310551214893, 'min_child_weight': 9, 'subsample': 0.7285444827637686, 'colsample_bytree': 0.6037485736963838, 'scale_pos_weight': 1.2771574457167314, 'gamma': 0.07884912418431478, 'reg_alpha': 1.064671675658094, 'reg_lambda': 4.9743597069439325, 'n_estimators': 208}. Best is trial 11 with value: 0.8615849128293896.\n",
      "[I 2025-04-26 11:57:09,490] Trial 13 finished with value: 0.8474781414864795 and parameters: {'max_depth': 8, 'learning_rate': 0.09923635331945362, 'min_child_weight': 7, 'subsample': 0.731393257098953, 'colsample_bytree': 0.6061515288559637, 'scale_pos_weight': 3.0452712956235555, 'gamma': 0.171461496307855, 'reg_alpha': 1.1793967234288083, 'reg_lambda': 3.506097036020316, 'n_estimators': 56}. Best is trial 11 with value: 0.8615849128293896.\n",
      "[I 2025-04-26 11:57:12,394] Trial 14 finished with value: 0.8268460767708158 and parameters: {'max_depth': 8, 'learning_rate': 0.09813277722587761, 'min_child_weight': 10, 'subsample': 0.7269156664998847, 'colsample_bytree': 0.6560676306043681, 'scale_pos_weight': 9.335659927817433, 'gamma': 0.898674977552545, 'reg_alpha': 2.943759838226633, 'reg_lambda': 3.8921832600849378, 'n_estimators': 231}. Best is trial 11 with value: 0.8615849128293896.\n",
      "[I 2025-04-26 11:57:14,844] Trial 15 finished with value: 0.8515528068881985 and parameters: {'max_depth': 9, 'learning_rate': 0.1209503113387313, 'min_child_weight': 7, 'subsample': 0.7865498509577621, 'colsample_bytree': 0.7333052412560193, 'scale_pos_weight': 2.963385187420581, 'gamma': 0.9352025246691104, 'reg_alpha': 1.4048717173462837, 'reg_lambda': 2.3407538545342867, 'n_estimators': 197}. Best is trial 11 with value: 0.8615849128293896.\n",
      "[I 2025-04-26 11:57:16,016] Trial 16 finished with value: 0.7648583671386527 and parameters: {'max_depth': 3, 'learning_rate': 0.0719226199722165, 'min_child_weight': 8, 'subsample': 0.993570501798496, 'colsample_bytree': 0.6377585810184367, 'scale_pos_weight': 9.028672579954286, 'gamma': 0.5372961561596334, 'reg_alpha': 2.6411628731989842, 'reg_lambda': 4.961614013251334, 'n_estimators': 249}. Best is trial 11 with value: 0.8615849128293896.\n",
      "[I 2025-04-26 11:57:18,027] Trial 17 finished with value: 0.8372793022437919 and parameters: {'max_depth': 7, 'learning_rate': 0.1511270075932136, 'min_child_weight': 9, 'subsample': 0.6092269440616355, 'colsample_bytree': 0.7076821871573088, 'scale_pos_weight': 1.5091844543173913, 'gamma': 1.788294187906518, 'reg_alpha': 0.4921335824103179, 'reg_lambda': 2.6073236976600556, 'n_estimators': 178}. Best is trial 11 with value: 0.8615849128293896.\n",
      "[I 2025-04-26 11:57:20,836] Trial 18 finished with value: 0.8592844622773385 and parameters: {'max_depth': 8, 'learning_rate': 0.19566936351936834, 'min_child_weight': 2, 'subsample': 0.7531200249985998, 'colsample_bytree': 0.7732518282921543, 'scale_pos_weight': 4.743167887936307, 'gamma': 4.973664040998725, 'reg_alpha': 1.4111953696135562, 'reg_lambda': 3.9764756843186824, 'n_estimators': 247}. Best is trial 11 with value: 0.8615849128293896.\n",
      "[I 2025-04-26 11:57:22,859] Trial 19 finished with value: 0.82232707659462 and parameters: {'max_depth': 8, 'learning_rate': 0.07290475966779042, 'min_child_weight': 1, 'subsample': 0.8560865757319067, 'colsample_bytree': 0.9935305488570373, 'scale_pos_weight': 4.710100565554528, 'gamma': 4.873698489214499, 'reg_alpha': 3.0304217904123965, 'reg_lambda': 3.8953010936709904, 'n_estimators': 298}. Best is trial 11 with value: 0.8615849128293896.\n",
      "[I 2025-04-26 11:57:22,862] A new study created in memory with name: no-name-f1e4f11d-8113-4af1-beec-ad83a9f4cb69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing LightGBM hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 11:57:24,722] Trial 0 finished with value: 0.46764122551741294 and parameters: {'n_estimators': 224, 'learning_rate': 0.0654785675746326, 'num_leaves': 57, 'max_depth': 8, 'min_child_samples': 27, 'subsample': 0.889338595340035, 'colsample_bytree': 0.6802681825524103, 'min_split_gain': 0.029981367940134687, 'min_child_weight': 0.030229440867681603, 'reg_alpha': 0.46979031341775224, 'reg_lambda': 0.43657966131130777}. Best is trial 0 with value: 0.46764122551741294.\n",
      "[I 2025-04-26 11:57:26,538] Trial 1 finished with value: 0.6325190966508335 and parameters: {'n_estimators': 105, 'learning_rate': 0.06633013094703986, 'num_leaves': 53, 'max_depth': 7, 'min_child_samples': 17, 'subsample': 0.7055047975567528, 'colsample_bytree': 0.9625396216724755, 'min_split_gain': 0.05566584525658741, 'min_child_weight': 0.004462513036113901, 'reg_alpha': 0.1474693855590381, 'reg_lambda': 0.6255933052437673}. Best is trial 1 with value: 0.6325190966508335.\n",
      "[I 2025-04-26 11:57:28,479] Trial 2 finished with value: 0.74323066096562 and parameters: {'n_estimators': 194, 'learning_rate': 0.16275545383455398, 'num_leaves': 43, 'max_depth': 4, 'min_child_samples': 30, 'subsample': 0.8547356139196229, 'colsample_bytree': 0.7010924766400347, 'min_split_gain': 0.07051538262741999, 'min_child_weight': 0.026402944636402217, 'reg_alpha': 0.6838718693351367, 'reg_lambda': 0.9566119489337375}. Best is trial 2 with value: 0.74323066096562.\n",
      "[I 2025-04-26 11:57:30,328] Trial 3 finished with value: 0.7107009054520068 and parameters: {'n_estimators': 245, 'learning_rate': 0.04514866614354646, 'num_leaves': 60, 'max_depth': 9, 'min_child_samples': 23, 'subsample': 0.7468710382807814, 'colsample_bytree': 0.7321852218572713, 'min_split_gain': 0.09407019237243434, 'min_child_weight': 0.03416474429643467, 'reg_alpha': 0.03741753502863121, 'reg_lambda': 0.09325307131012928}. Best is trial 2 with value: 0.74323066096562.\n",
      "[I 2025-04-26 11:57:32,415] Trial 4 finished with value: 0.7330806488481958 and parameters: {'n_estimators': 142, 'learning_rate': 0.019495318844138878, 'num_leaves': 24, 'max_depth': 3, 'min_child_samples': 17, 'subsample': 0.7423108274657518, 'colsample_bytree': 0.6727878801923877, 'min_split_gain': 0.09482949467206356, 'min_child_weight': 0.0023703895091843276, 'reg_alpha': 0.9682530426923448, 'reg_lambda': 0.52915746921381}. Best is trial 2 with value: 0.74323066096562.\n",
      "[I 2025-04-26 11:57:34,553] Trial 5 finished with value: 0.7551333031679093 and parameters: {'n_estimators': 197, 'learning_rate': 0.0169344695060836, 'num_leaves': 17, 'max_depth': 4, 'min_child_samples': 29, 'subsample': 0.9879739734095312, 'colsample_bytree': 0.7253520744689113, 'min_split_gain': 0.05712090432531968, 'min_child_weight': 0.0013878103945469344, 'reg_alpha': 0.7280626570654329, 'reg_lambda': 0.568701944454247}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:36,411] Trial 6 finished with value: 0.5944025281407262 and parameters: {'n_estimators': 103, 'learning_rate': 0.050072513728438066, 'num_leaves': 43, 'max_depth': 7, 'min_child_samples': 9, 'subsample': 0.7882098614517712, 'colsample_bytree': 0.948170443539675, 'min_split_gain': 0.08940943646741467, 'min_child_weight': 0.0769214651877494, 'reg_alpha': 0.8822744097454484, 'reg_lambda': 0.20419112635526881}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:38,760] Trial 7 finished with value: 0.6218852040718424 and parameters: {'n_estimators': 296, 'learning_rate': 0.027142991752054927, 'num_leaves': 54, 'max_depth': 5, 'min_child_samples': 13, 'subsample': 0.8031389334227985, 'colsample_bytree': 0.946037024170522, 'min_split_gain': 0.04229618741604216, 'min_child_weight': 0.002328032859216158, 'reg_alpha': 0.30337954939509715, 'reg_lambda': 0.4296909493310379}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:40,968] Trial 8 finished with value: 0.7527467439297656 and parameters: {'n_estimators': 50, 'learning_rate': 0.1334335776953664, 'num_leaves': 49, 'max_depth': 5, 'min_child_samples': 29, 'subsample': 0.8226248662493922, 'colsample_bytree': 0.799888954713561, 'min_split_gain': 0.04345009519741098, 'min_child_weight': 0.03669446461099158, 'reg_alpha': 0.051838160924358556, 'reg_lambda': 0.3470061353197047}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:42,987] Trial 9 finished with value: 0.4820671605109054 and parameters: {'n_estimators': 127, 'learning_rate': 0.017400242129753593, 'num_leaves': 21, 'max_depth': 9, 'min_child_samples': 9, 'subsample': 0.6523530414262324, 'colsample_bytree': 0.969815276099368, 'min_split_gain': 0.04054304656010915, 'min_child_weight': 0.03407590017121023, 'reg_alpha': 0.9826650054536689, 'reg_lambda': 0.756286600509493}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:44,991] Trial 10 finished with value: 0.74150231781289 and parameters: {'n_estimators': 293, 'learning_rate': 0.010081020533857149, 'num_leaves': 15, 'max_depth': 3, 'min_child_samples': 23, 'subsample': 0.9889119844256421, 'colsample_bytree': 0.6110307330796237, 'min_split_gain': 0.015088594911246152, 'min_child_weight': 0.0010251257871712028, 'reg_alpha': 0.6973106648253954, 'reg_lambda': 0.7675589548869804}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:47,056] Trial 11 finished with value: 0.6878709016707812 and parameters: {'n_estimators': 52, 'learning_rate': 0.19188344708645066, 'num_leaves': 33, 'max_depth': 5, 'min_child_samples': 30, 'subsample': 0.9966612365804803, 'colsample_bytree': 0.8359737669327931, 'min_split_gain': 0.06180786481511122, 'min_child_weight': 0.00866511993310366, 'reg_alpha': 0.5444697421378841, 'reg_lambda': 0.2764132150581977}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:49,151] Trial 12 finished with value: 0.7401217186339418 and parameters: {'n_estimators': 52, 'learning_rate': 0.1148882732844242, 'num_leaves': 34, 'max_depth': 5, 'min_child_samples': 24, 'subsample': 0.9253877063299553, 'colsample_bytree': 0.8130551965818469, 'min_split_gain': 0.0015811732004418766, 'min_child_weight': 0.011343271704936908, 'reg_alpha': 0.3153232436196226, 'reg_lambda': 0.30789757000130213}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:50,987] Trial 13 finished with value: 0.7487798618516776 and parameters: {'n_estimators': 181, 'learning_rate': 0.1043306012813904, 'num_leaves': 46, 'max_depth': 4, 'min_child_samples': 26, 'subsample': 0.6035318483720313, 'colsample_bytree': 0.7698199292971967, 'min_split_gain': 0.07694425934173707, 'min_child_weight': 0.012614857561734989, 'reg_alpha': 0.7991889605370114, 'reg_lambda': 0.007172331572892099}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:52,973] Trial 14 finished with value: 0.5733428210779637 and parameters: {'n_estimators': 222, 'learning_rate': 0.010152568433524072, 'num_leaves': 28, 'max_depth': 6, 'min_child_samples': 21, 'subsample': 0.9508243119792995, 'colsample_bytree': 0.8614460901791499, 'min_split_gain': 0.029028347493713406, 'min_child_weight': 0.07828443368983273, 'reg_alpha': 0.49612253701564135, 'reg_lambda': 0.606279245074768}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:54,848] Trial 15 finished with value: 0.682659435749647 and parameters: {'n_estimators': 135, 'learning_rate': 0.02470457817099319, 'num_leaves': 47, 'max_depth': 4, 'min_child_samples': 28, 'subsample': 0.8547648537768016, 'colsample_bytree': 0.8916744928389343, 'min_split_gain': 0.0480214084238539, 'min_child_weight': 0.005514094898210711, 'reg_alpha': 0.646935956221212, 'reg_lambda': 0.37279397937351655}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:56,863] Trial 16 finished with value: 0.7372877383663495 and parameters: {'n_estimators': 257, 'learning_rate': 0.032075835814734416, 'num_leaves': 15, 'max_depth': 6, 'min_child_samples': 20, 'subsample': 0.9237411477953035, 'colsample_bytree': 0.7494565811080776, 'min_split_gain': 0.07071441758363037, 'min_child_weight': 0.0014331113743850583, 'reg_alpha': 0.3114228854334031, 'reg_lambda': 0.7091097159048185}. Best is trial 5 with value: 0.7551333031679093.\n",
      "[I 2025-04-26 11:57:58,771] Trial 17 finished with value: 0.7579815435525156 and parameters: {'n_estimators': 159, 'learning_rate': 0.09980175373382087, 'num_leaves': 38, 'max_depth': 4, 'min_child_samples': 6, 'subsample': 0.8300459260095514, 'colsample_bytree': 0.6220659970084006, 'min_split_gain': 0.029201416826979146, 'min_child_weight': 0.051813630794801155, 'reg_alpha': 0.04597710900078354, 'reg_lambda': 0.935151372397868}. Best is trial 17 with value: 0.7579815435525156.\n",
      "[I 2025-04-26 11:58:00,886] Trial 18 finished with value: 0.7598892009455182 and parameters: {'n_estimators': 168, 'learning_rate': 0.08289893549823593, 'num_leaves': 37, 'max_depth': 3, 'min_child_samples': 5, 'subsample': 0.9557813071628072, 'colsample_bytree': 0.6091969753173453, 'min_split_gain': 0.02228718089156831, 'min_child_weight': 0.018380831967552268, 'reg_alpha': 0.18939587437176686, 'reg_lambda': 0.9696893144321871}. Best is trial 18 with value: 0.7598892009455182.\n",
      "[I 2025-04-26 11:58:02,854] Trial 19 finished with value: 0.74922585115741 and parameters: {'n_estimators': 155, 'learning_rate': 0.08436686486062336, 'num_leaves': 39, 'max_depth': 3, 'min_child_samples': 5, 'subsample': 0.8825532896297943, 'colsample_bytree': 0.6005991481292554, 'min_split_gain': 0.0157236723604322, 'min_child_weight': 0.01818271684372123, 'reg_alpha': 0.17774753481019856, 'reg_lambda': 0.962269505444452}. Best is trial 18 with value: 0.7598892009455182.\n",
      "[I 2025-04-26 11:58:02,857] A new study created in memory with name: no-name-4a254a0b-28c4-4b16-b234-c9121cfb5016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing CatBoost hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 11:58:19,862] Trial 0 finished with value: 0.8493955228090125 and parameters: {'iterations': 101, 'learning_rate': 0.03328174939123163, 'depth': 9, 'l2_leaf_reg': 6.142078734395756, 'border_count': 85, 'scale_pos_weight': 12.594178404485636}. Best is trial 0 with value: 0.8493955228090125.\n",
      "[I 2025-04-26 11:58:36,059] Trial 1 finished with value: 0.814155457682486 and parameters: {'iterations': 149, 'learning_rate': 0.011702042903440494, 'depth': 5, 'l2_leaf_reg': 8.975379465087688, 'border_count': 80, 'scale_pos_weight': 16.15654390665393}. Best is trial 0 with value: 0.8493955228090125.\n",
      "[I 2025-04-26 11:58:53,542] Trial 2 finished with value: 0.8255009017160789 and parameters: {'iterations': 210, 'learning_rate': 0.0317196240293269, 'depth': 4, 'l2_leaf_reg': 9.467942986580788, 'border_count': 126, 'scale_pos_weight': 7.779203687716522}. Best is trial 0 with value: 0.8493955228090125.\n",
      "[I 2025-04-26 11:59:11,366] Trial 3 finished with value: 0.8551644760029743 and parameters: {'iterations': 285, 'learning_rate': 0.05150163063342374, 'depth': 6, 'l2_leaf_reg': 8.928870742466763, 'border_count': 119, 'scale_pos_weight': 2.989416579646989}. Best is trial 3 with value: 0.8551644760029743.\n",
      "[I 2025-04-26 11:59:25,656] Trial 4 finished with value: 0.8542307914961578 and parameters: {'iterations': 55, 'learning_rate': 0.1125369324436799, 'depth': 7, 'l2_leaf_reg': 4.235207937433882, 'border_count': 69, 'scale_pos_weight': 1.7901626293612}. Best is trial 3 with value: 0.8551644760029743.\n",
      "[I 2025-04-26 11:59:42,843] Trial 5 finished with value: 0.8475403612677622 and parameters: {'iterations': 83, 'learning_rate': 0.015305894839977612, 'depth': 9, 'l2_leaf_reg': 1.2283789493614672, 'border_count': 113, 'scale_pos_weight': 19.078007350862514}. Best is trial 3 with value: 0.8551644760029743.\n",
      "[I 2025-04-26 12:00:03,865] Trial 6 finished with value: 0.8576576399857073 and parameters: {'iterations': 268, 'learning_rate': 0.021184929404906187, 'depth': 8, 'l2_leaf_reg': 9.547259009612503, 'border_count': 61, 'scale_pos_weight': 4.204156082976235}. Best is trial 6 with value: 0.8576576399857073.\n",
      "[I 2025-04-26 12:00:17,421] Trial 7 finished with value: 0.8255689182687945 and parameters: {'iterations': 63, 'learning_rate': 0.02213004233057471, 'depth': 5, 'l2_leaf_reg': 5.5249794782436545, 'border_count': 66, 'scale_pos_weight': 7.0046994914224365}. Best is trial 6 with value: 0.8576576399857073.\n",
      "[I 2025-04-26 12:00:30,972] Trial 8 finished with value: 0.8197043120837808 and parameters: {'iterations': 111, 'learning_rate': 0.062140985601211354, 'depth': 5, 'l2_leaf_reg': 4.983156459999572, 'border_count': 112, 'scale_pos_weight': 16.523789655586928}. Best is trial 6 with value: 0.8576576399857073.\n",
      "[I 2025-04-26 12:00:46,307] Trial 9 finished with value: 0.8384374807447164 and parameters: {'iterations': 151, 'learning_rate': 0.04080821400220234, 'depth': 6, 'l2_leaf_reg': 9.183895098193453, 'border_count': 105, 'scale_pos_weight': 15.913882936295543}. Best is trial 6 with value: 0.8576576399857073.\n",
      "[I 2025-04-26 12:00:59,140] Trial 10 finished with value: 0.8466586859458504 and parameters: {'iterations': 300, 'learning_rate': 0.19042360359996793, 'depth': 8, 'l2_leaf_reg': 7.258019495262572, 'border_count': 34, 'scale_pos_weight': 5.063165664038246}. Best is trial 6 with value: 0.8576576399857073.\n",
      "[I 2025-04-26 12:01:16,570] Trial 11 finished with value: 0.8649449348215015 and parameters: {'iterations': 279, 'learning_rate': 0.06466488234944177, 'depth': 7, 'l2_leaf_reg': 7.824705022736504, 'border_count': 47, 'scale_pos_weight': 1.0858456387998139}. Best is trial 11 with value: 0.8649449348215015.\n",
      "[I 2025-04-26 12:01:33,249] Trial 12 finished with value: 0.8568846508938103 and parameters: {'iterations': 244, 'learning_rate': 0.07544482904855943, 'depth': 8, 'l2_leaf_reg': 7.509749992281894, 'border_count': 42, 'scale_pos_weight': 1.3702909829924739}. Best is trial 11 with value: 0.8649449348215015.\n",
      "[I 2025-04-26 12:01:50,957] Trial 13 finished with value: 0.8455357798018598 and parameters: {'iterations': 254, 'learning_rate': 0.020803068250948607, 'depth': 7, 'l2_leaf_reg': 7.518240718449787, 'border_count': 51, 'scale_pos_weight': 9.848573652520344}. Best is trial 11 with value: 0.8649449348215015.\n",
      "[I 2025-04-26 12:02:04,797] Trial 14 finished with value: 0.8544082328879519 and parameters: {'iterations': 207, 'learning_rate': 0.10008479215532197, 'depth': 8, 'l2_leaf_reg': 3.4703992362047362, 'border_count': 57, 'scale_pos_weight': 4.438328572919181}. Best is trial 11 with value: 0.8649449348215015.\n",
      "[I 2025-04-26 12:02:23,999] Trial 15 finished with value: 0.85629682529252 and parameters: {'iterations': 253, 'learning_rate': 0.023922514638291394, 'depth': 7, 'l2_leaf_reg': 9.985513997229429, 'border_count': 47, 'scale_pos_weight': 5.6013204830004915}. Best is trial 11 with value: 0.8649449348215015.\n",
      "[I 2025-04-26 12:02:36,864] Trial 16 finished with value: 0.8280348846100377 and parameters: {'iterations': 212, 'learning_rate': 0.16620582819026283, 'depth': 8, 'l2_leaf_reg': 8.032516954732163, 'border_count': 32, 'scale_pos_weight': 9.218035459020296}. Best is trial 11 with value: 0.8649449348215015.\n",
      "[I 2025-04-26 12:03:04,559] Trial 17 finished with value: 0.8573935320429135 and parameters: {'iterations': 272, 'learning_rate': 0.01448812895934306, 'depth': 9, 'l2_leaf_reg': 6.464709277545689, 'border_count': 95, 'scale_pos_weight': 3.775816696338686}. Best is trial 11 with value: 0.8649449348215015.\n",
      "[I 2025-04-26 12:03:18,131] Trial 18 finished with value: 0.8368814535282995 and parameters: {'iterations': 187, 'learning_rate': 0.08650789718058374, 'depth': 6, 'l2_leaf_reg': 8.103142251901472, 'border_count': 62, 'scale_pos_weight': 12.089746831623724}. Best is trial 11 with value: 0.8649449348215015.\n",
      "[I 2025-04-26 12:03:33,916] Trial 19 finished with value: 0.8563841445709665 and parameters: {'iterations': 245, 'learning_rate': 0.04774429363698269, 'depth': 7, 'l2_leaf_reg': 8.52664358308439, 'border_count': 70, 'scale_pos_weight': 6.60932427410935}. Best is trial 11 with value: 0.8649449348215015.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST HYPERPARAMETERS ===\n",
      "Best XGBoost parameters: {'max_depth': 9, 'learning_rate': 0.19204590754392462, 'min_child_weight': 8, 'subsample': 0.7395244220349267, 'colsample_bytree': 0.6106090410074912, 'scale_pos_weight': 1.5603241849262437, 'gamma': 0.5971932711844856, 'reg_alpha': 1.1856237564368213, 'reg_lambda': 4.949550158662442, 'n_estimators': 218}\n",
      "Best XGBoost PR-AUC: 0.8615849128293896\n",
      "\n",
      "Best LightGBM parameters: {'n_estimators': 168, 'learning_rate': 0.08289893549823593, 'num_leaves': 37, 'max_depth': 3, 'min_child_samples': 5, 'subsample': 0.9557813071628072, 'colsample_bytree': 0.6091969753173453, 'min_split_gain': 0.02228718089156831, 'min_child_weight': 0.018380831967552268, 'reg_alpha': 0.18939587437176686, 'reg_lambda': 0.9696893144321871}\n",
      "Best LightGBM PR-AUC: 0.7598892009455182\n",
      "\n",
      "Best CatBoost parameters: {'iterations': 279, 'learning_rate': 0.06466488234944177, 'depth': 7, 'l2_leaf_reg': 7.824705022736504, 'border_count': 47, 'scale_pos_weight': 1.0858456387998139}\n",
      "Best CatBoost PR-AUC: 0.8649449348215015\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "\n",
    "print(\"\\n=== STARTING MODEL OPTIMIZATION ===\")\n",
    "\n",
    "# Force conversion to numpy arrays before optimization\n",
    "def prepare_data(X, y):\n",
    "    \"\"\"Convert data to numpy arrays to avoid indexing issues\"\"\"\n",
    "    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "        X = X.values\n",
    "    if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
    "        y = y.values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "n_trials = 35  \n",
    "\n",
    "def objective_xgb(trial):\n",
    "    \"\"\"Objective function for XGBoost optimization using lower-level XGBoost API\"\"\"\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'eta': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 20.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        'alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n",
    "        'lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'aucpr',\n",
    "        'seed': 42\n",
    "    }\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    \n",
    "    try:\n",
    "        # Get data as numpy arrays\n",
    "        X_data, y_data = prepare_data(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        # Cross-validation\n",
    "        scores = []\n",
    "        \n",
    "        # 3 FOLDS CROSS VALIDATIONs\n",
    "        kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        for train_index, val_index in kf.split(X_data, y_data):\n",
    "            X_t, X_v = X_data[train_index], X_data[val_index]\n",
    "            y_t, y_v = y_data[train_index], y_data[val_index]\n",
    "            \n",
    "            try:\n",
    "                dtrain = xgb.DMatrix(X_t, label=y_t)\n",
    "                dval = xgb.DMatrix(X_v, label=y_v)\n",
    "                \n",
    "                # Train model with early stopping\n",
    "                model = xgb.train(\n",
    "                    param,\n",
    "                    dtrain,\n",
    "                    num_boost_round=n_estimators,\n",
    "                    evals=[(dval, 'validation')],\n",
    "                    early_stopping_rounds=10,\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(dval)\n",
    "                score = average_precision_score(y_v, y_pred)\n",
    "                scores.append(score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during XGBoost training fold: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not scores:\n",
    "            return 0.0\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in XGBoost objective function: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def objective_lgbm(trial):\n",
    "    \"\"\"Objective function for LightGBM optimization\"\"\"\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 60),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.1),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 0.1, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'class_weight': 'balanced',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': 4,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get data as numpy arrays to avoid indexing issues\n",
    "        X_data, y_data = prepare_data(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        # 3 FOLD Cross-validation\n",
    "        scores = []\n",
    "        kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        for train_index, val_index in kf.split(X_data, y_data):\n",
    "            X_t, X_v = X_data[train_index], X_data[val_index]\n",
    "            y_t, y_v = y_data[train_index], y_data[val_index]\n",
    "            \n",
    "            try:\n",
    "                model = LGBMClassifier(**param)\n",
    "                model.fit(\n",
    "                    X_t, y_t,\n",
    "                    eval_set=[(X_v, y_v)],\n",
    "                    eval_metric='auc', \n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n",
    "                )\n",
    "                \n",
    "                y_pred = model.predict_proba(X_v)[:, 1]\n",
    "                score = average_precision_score(y_v, y_pred)\n",
    "                scores.append(score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during LightGBM training fold: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not scores:\n",
    "            return 0.0  \n",
    "        return np.mean(scores)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LightGBM objective function: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def objective_catboost(trial):\n",
    "    \"\"\"Objective function for CatBoost optimization\"\"\"\n",
    "    param = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'depth': trial.suggest_int('depth', 4, 9),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 128),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 20.0),\n",
    "        'random_seed': 42,\n",
    "        'thread_count': 4,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get data as numpy arrays to avoid indexing issues\n",
    "        X_data, y_data = prepare_data(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        # 3 FOLD Cross-validation\n",
    "        scores = []\n",
    "        kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        for train_index, val_index in kf.split(X_data, y_data):\n",
    "            X_t, X_v = X_data[train_index], X_data[val_index]\n",
    "            y_t, y_v = y_data[train_index], y_data[val_index]\n",
    "            \n",
    "            try:\n",
    "                model = CatBoostClassifier(**param)\n",
    "                model.fit(\n",
    "                    X_t, y_t,\n",
    "                    eval_set=[(X_v, y_v)],\n",
    "                    early_stopping_rounds=10,\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                y_pred = model.predict_proba(X_v)[:, 1]\n",
    "                score = average_precision_score(y_v, y_pred)\n",
    "                scores.append(score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during CatBoost training fold: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not scores:\n",
    "            return 0.0  \n",
    "        return np.mean(scores)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in CatBoost objective function: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "try:\n",
    "    X_train_resampled_shape = X_train_resampled.shape\n",
    "    y_train_resampled_shape = y_train_resampled.shape\n",
    "    print(f\"Data loaded successfully. X shape: {X_train_resampled_shape}, y shape: {y_train_resampled_shape}\")\n",
    "except NameError:\n",
    "    print(\"\\nERROR: X_train_resampled and/or y_train_resampled not found.\")\n",
    "    print(\"Please define these variables before running the optimization.\")\n",
    "    print(\"Exiting...\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Using Optuna for hyperparameter optimization\n",
    "print(\"\\nOptimizing XGBoost hyperparameters...\")\n",
    "study_xgb = optuna.create_study(direction='maximize', \n",
    "                              pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study_xgb.optimize(objective_xgb, n_trials=n_trials)\n",
    "\n",
    "print(\"\\nOptimizing LightGBM hyperparameters...\")\n",
    "study_lgbm = optuna.create_study(direction='maximize',\n",
    "                               pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=n_trials)\n",
    "\n",
    "print(\"\\nOptimizing CatBoost hyperparameters...\")\n",
    "study_catboost = optuna.create_study(direction='maximize',\n",
    "                                   pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study_catboost.optimize(objective_catboost, n_trials=n_trials)\n",
    "\n",
    "# Display best parameters\n",
    "print(\"\\n=== BEST HYPERPARAMETERS ===\")\n",
    "print(\"Best XGBoost parameters:\", study_xgb.best_params)\n",
    "print(\"Best XGBoost PR-AUC:\", study_xgb.best_value)\n",
    "\n",
    "print(\"\\nBest LightGBM parameters:\", study_lgbm.best_params)\n",
    "print(\"Best LightGBM PR-AUC:\", study_lgbm.best_value)\n",
    "\n",
    "print(\"\\nBest CatBoost parameters:\", study_catboost.best_params)\n",
    "print(\"Best CatBoost PR-AUC:\", study_catboost.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa7a0e",
   "metadata": {},
   "source": [
    " TRAINING FINAL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14157a13-4368-40b5-a9a6-ecc2e3e90b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAINING FINAL MODELS ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x17f6f44d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"\\n=== TRAINING FINAL MODELS ===\")\n",
    "\n",
    "# Create validation set\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42, stratify=y_train_resampled\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "xgb_params = study_xgb.best_params.copy()\n",
    "n_estimators_xgb = xgb_params.pop('n_estimators', 300)\n",
    "\n",
    "xgb_model = XGBClassifier(n_estimators=n_estimators_xgb, **xgb_params, n_jobs=4)\n",
    "xgb_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# LightGBM\n",
    "lgbm_params = study_lgbm.best_params.copy()\n",
    "n_estimators_lgbm = lgbm_params.pop('n_estimators', 300)\n",
    "lgbm_model = LGBMClassifier(n_estimators=n_estimators_lgbm, **lgbm_params, n_jobs=4)\n",
    "lgbm_model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n",
    ")\n",
    "\n",
    "#  CatBoost\n",
    "catboost_params = study_catboost.best_params.copy()\n",
    "iterations = catboost_params.pop('iterations', 300)\n",
    "catboost_model = CatBoostClassifier(iterations=iterations, **catboost_params, thread_count=4)\n",
    "catboost_model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8121b63e",
   "metadata": {},
   "source": [
    "MODEL EVALUATION\n",
    "\n",
    "find_optimal_threshold: Finds the threshold that maximizes the F1 score using the precision-recall curve.\n",
    "\n",
    "evaluate_model: Evaluates the model using several metrics including ROC AUC, PR AUC, Precision, Recall, F1 score, and confusion matrix.\n",
    "\n",
    "XGBoost: Evaluated using the test data (X_test_with_pca, y_test), output includes metrics like ROC AUC, PR AUC, Precision, Recall, F1 score, and confusion matrix.\n",
    "\n",
    "LightGBM: Same evaluation process as XGBoost.\n",
    "\n",
    "CatBoost: Same evaluation process as XGBoost.\n",
    "\n",
    "Ensemble Creation: A weighted average of model predictions (XGBoost, LightGBM, CatBoost) based on their validation PR AUC scores.\n",
    "\n",
    "Ensemble Weights: Calculated based on validation PR AUC\n",
    "\n",
    "Ensemble Metrics: The weighted ensemble is evaluated using the same metrics as individual models, including ROC AUC, PR AUC, Precision, Recall, F1 score, and confusion matrix.\n",
    "\n",
    "Optimal Threshold for Ensemble: Calculated using the F1 score.\n",
    "\n",
    "Ensemble Results: Printed for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd0af2d-7973-4594-95b0-72957d587338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATING MODELS ===\n",
      "\n",
      "XGBoost Model Evaluation:\n",
      "ROC AUC: 0.9779\n",
      "PR AUC: 0.8282\n",
      "Optimal threshold: 0.4526\n",
      "Precision: 0.9328\n",
      "Recall: 0.7500\n",
      "F1 Score: 0.8315\n",
      "\n",
      "LightGBM Model Evaluation:\n",
      "ROC AUC: 0.9620\n",
      "PR AUC: 0.7923\n",
      "Optimal threshold: 0.5320\n",
      "Precision: 0.9000\n",
      "Recall: 0.7297\n",
      "F1 Score: 0.8060\n",
      "\n",
      "CatBoost Model Evaluation:\n",
      "ROC AUC: 0.9774\n",
      "PR AUC: 0.8208\n",
      "Optimal threshold: 0.2302\n",
      "Precision: 0.9084\n",
      "Recall: 0.8041\n",
      "F1 Score: 0.8530\n",
      "\n",
      "Ensemble weights:\n",
      "XGBoost: 0.3407\n",
      "LightGBM: 0.3164\n",
      "CatBoost: 0.3429\n",
      "\n",
      "Weighted Ensemble Model Evaluation:\n",
      "ROC AUC: 0.9799\n",
      "PR AUC: 0.8296\n",
      "Optimal threshold: 0.3521\n",
      "Precision: 0.9062\n",
      "Recall: 0.7838\n",
      "F1 Score: 0.8406\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== EVALUATING MODELS ===\")\n",
    "\n",
    "# Function to find optimal threshold using F1 score\n",
    "def find_optimal_threshold(y_true, y_score):\n",
    "    \"\"\"Find the optimal threshold that maximizes F1 score\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "    f1_scores = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "    if len(thresholds) < len(f1_scores):\n",
    "        thresholds = np.append(thresholds, 1.0)\n",
    "    return thresholds[np.argmax(f1_scores)], np.max(f1_scores)\n",
    "\n",
    "# Function to evaluate and visualize model performance\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model and return predictions and metrics\"\"\"\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    pr_auc = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    # Finding optimal threshold\n",
    "    optimal_threshold, best_f1 = find_optimal_threshold(y_test, y_prob)\n",
    "    y_pred = (y_prob >= optimal_threshold).astype(int)\n",
    "    \n",
    "    # Calculation of  metrics\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Model Evaluation:\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "    print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'y_prob': y_prob,\n",
    "        'y_pred': y_pred,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Evaluating  each model\n",
    "xgb_results = evaluate_model(xgb_model, X_test_with_pca, y_test, \"XGBoost\")\n",
    "lgbm_results = evaluate_model(lgbm_model, X_test_with_pca, y_test, \"LightGBM\")\n",
    "catboost_results = evaluate_model(catboost_model, X_test_with_pca, y_test, \"CatBoost\")\n",
    "\n",
    "\n",
    "# Calculating weights based on validation PR-AUC\n",
    "xgb_val_probs = xgb_model.predict_proba(X_val)[:, 1]\n",
    "lgbm_val_probs = lgbm_model.predict_proba(X_val)[:, 1]\n",
    "catboost_val_probs = catboost_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "xgb_val_pr_auc = average_precision_score(y_val, xgb_val_probs)\n",
    "lgbm_val_pr_auc = average_precision_score(y_val, lgbm_val_probs)\n",
    "catboost_val_pr_auc = average_precision_score(y_val, catboost_val_probs)\n",
    "\n",
    "# Calculating weighted ensemble weights based on PR-AUC\n",
    "total_pr_auc = xgb_val_pr_auc + lgbm_val_pr_auc + catboost_val_pr_auc\n",
    "weights = [\n",
    "    xgb_val_pr_auc / total_pr_auc,\n",
    "    lgbm_val_pr_auc / total_pr_auc,\n",
    "    catboost_val_pr_auc / total_pr_auc\n",
    "]\n",
    "\n",
    "print(f\"\\nEnsemble weights:\")\n",
    "print(f\"XGBoost: {weights[0]:.4f}\")\n",
    "print(f\"LightGBM: {weights[1]:.4f}\")\n",
    "print(f\"CatBoost: {weights[2]:.4f}\")\n",
    "\n",
    "# Creating ensemble predictions\n",
    "ensemble_probs = (\n",
    "    weights[0] * xgb_results['y_prob'] + \n",
    "    weights[1] * lgbm_results['y_prob'] + \n",
    "    weights[2] * catboost_results['y_prob']\n",
    ")\n",
    "\n",
    "# Evaluating ensemble\n",
    "ensemble_results = {\n",
    "    'model_name': 'Weighted Ensemble',\n",
    "    'y_prob': ensemble_probs\n",
    "}\n",
    "\n",
    "# Finding optimal threshold for ensemble\n",
    "optimal_threshold, _ = find_optimal_threshold(y_test, ensemble_probs)\n",
    "ensemble_results['optimal_threshold'] = optimal_threshold\n",
    "ensemble_results['y_pred'] = (ensemble_probs >= optimal_threshold).astype(int)\n",
    "\n",
    "# Calculating metrics for ensemble\n",
    "ensemble_results['roc_auc'] = roc_auc_score(y_test, ensemble_probs)\n",
    "ensemble_results['pr_auc'] = average_precision_score(y_test, ensemble_probs)\n",
    "ensemble_results['precision'] = precision_score(y_test, ensemble_results['y_pred'])\n",
    "ensemble_results['recall'] = recall_score(y_test, ensemble_results['y_pred'])\n",
    "ensemble_results['f1'] = f1_score(y_test, ensemble_results['y_pred'])\n",
    "ensemble_results['confusion_matrix'] = confusion_matrix(y_test, ensemble_results['y_pred'])\n",
    "\n",
    "print(f\"\\nWeighted Ensemble Model Evaluation:\")\n",
    "print(f\"ROC AUC: {ensemble_results['roc_auc']:.4f}\")\n",
    "print(f\"PR AUC: {ensemble_results['pr_auc']:.4f}\")\n",
    "print(f\"Optimal threshold: {ensemble_results['optimal_threshold']:.4f}\")\n",
    "print(f\"Precision: {ensemble_results['precision']:.4f}\")\n",
    "print(f\"Recall: {ensemble_results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {ensemble_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde195a-2fe6-4e39-b821-5f582588cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "results = [\n",
    "    {'Model': 'XGBoost', 'PR AUC': 0.8282, 'Threshold': 0.4526},\n",
    "    {'Model': 'LightGBM', 'PR AUC': 0.7923, 'Threshold': 0.5320},\n",
    "    {'Model': 'CatBoost', 'PR AUC': 0.8208, 'Threshold': 0.2302},\n",
    "    {'Model': 'Weighted Ensemble', 'PR AUC': 0.8296, 'Threshold': 0.3521}\n",
    "]\n",
    "\n",
    "# Create the DataFrame and sort by PR AUC descending\n",
    "results_df = pd.DataFrame(results).sort_values(by='PR AUC', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba071408-9391-4ce2-8028-bd54a9190571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAVING THE BEST MODEL ===\n",
      "Best model based on PR-AUC: Weighted Ensemble\n",
      "Ensemble model saved as 'ensemble_models.pkl'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== SAVING THE BEST MODEL ===\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Identify the best model based on PR-AUC\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"Best model based on PR-AUC: {best_model_name}\")\n",
    "\n",
    "# Save the corresponding model\n",
    "best_model = None\n",
    "if best_model_name == 'XGBoost':\n",
    "    best_model = xgb_model\n",
    "elif best_model_name == 'LightGBM':\n",
    "    best_model = lgbm_model\n",
    "elif best_model_name == 'CatBoost':\n",
    "    best_model = catboost_model\n",
    "elif best_model_name == 'Weighted Ensemble':\n",
    "    # For ensemble, save all component models and weights\n",
    "    with open('ensemble_models.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'xgb_model': xgb_model,\n",
    "            'lgbm_model': lgbm_model,\n",
    "            'catboost_model': catboost_model,\n",
    "            'weights': weights,\n",
    "            'threshold': ensemble_results['optimal_threshold'],\n",
    "            'feature_engineer': feature_engineer,\n",
    "            'preprocess_pipeline': preprocess_pipeline,\n",
    "            'pca': pca\n",
    "        }, f)\n",
    "    print(\"Ensemble model saved as 'ensemble_models.pkl'\")\n",
    "    best_model = 'ensemble'\n",
    "\n",
    "# If a single model was best\n",
    "if best_model != 'ensemble' and best_model is not None:\n",
    "    with open(f'{best_model_name.lower()}_model.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': best_model,\n",
    "            'threshold': results_df.iloc[0]['Threshold'],\n",
    "            'feature_engineer': feature_engineer,\n",
    "            'preprocess_pipeline': preprocess_pipeline,\n",
    "            'pca': pca\n",
    "        }, f)\n",
    "    print(f\"{best_model_name} model saved as '{best_model_name.lower()}_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe6785-134a-4ea2-b242-f4ef12530355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
